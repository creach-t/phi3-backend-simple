# üîß Guide d'installation et de configuration

## Table des mati√®res
- [Pr√©requis](#pr√©requis)
- [Installation](#installation)
- [Configuration](#configuration)
- [llama.cpp](#llamacpp)
- [Premier d√©marrage](#premier-d√©marrage)
- [D√©pannage](#d√©pannage)

## Pr√©requis

### Syst√®me requis
- **OS** : Linux, macOS, ou Windows (avec WSL recommand√©)
- **Node.js** : Version 18.0 ou sup√©rieure
- **RAM** : Minimum 4GB (8GB+ recommand√© pour les mod√®les)
- **Stockage** : 10GB+ libres (pour les mod√®les)
- **CPU** : Compatible avec llama.cpp (x86_64, ARM64)

### Outils n√©cessaires
```bash
# V√©rifier Node.js
node --version  # Doit √™tre >= 18.0

# Installer les outils de build (si n√©cessaire)
# Ubuntu/Debian
sudo apt update
sudo apt install build-essential git curl

# macOS
xcode-select --install

# Windows (PowerShell en admin)
# Installer Visual Studio Build Tools ou Visual Studio Community
```

## Installation

### 1. Cloner le repository
```bash
git clone https://github.com/creach-t/phi3-backend-simple.git
cd phi3-backend-simple
```

### 2. Installer les d√©pendances
```bash
# Avec npm
npm install

# Ou avec yarn
yarn install
```

### 3. Configuration de base
```bash
# Copier le fichier d'environnement
cp .env.example .env

# √âditer la configuration
nano .env  # ou code .env
```

## Configuration

### Variables d'environnement (.env)

#### Serveur
```bash
# Port d'√©coute du serveur
PORT=3000

# Environnement (development, production, test)
NODE_ENV=development
```

#### JWT (IMPORTANT pour la s√©curit√©)
```bash
# Secret pour signer les tokens JWT
# ‚ö†Ô∏è CHANGER ABSOLUMENT pour la production
JWT_SECRET=votre-super-secret-jwt-key-tres-securisee

# Dur√©e de validit√© des tokens en secondes (24h = 86400)
JWT_EXPIRES_IN=86400
```

**G√©n√©rer une cl√© s√©curis√©e :**
```bash
# M√©thode 1 : OpenSSL
openssl rand -base64 32

# M√©thode 2 : Node.js
node -e "console.log(require('crypto').randomBytes(32).toString('hex'))"

# M√©thode 3 : En ligne
# https://generate-secret.vercel.app/32
```

#### Base de donn√©es
```bash
# Chemin vers la base SQLite
DB_PATH=./database.sqlite

# Pour production, utiliser un chemin absolu
# DB_PATH=/var/lib/phi3-backend/database.sqlite
```

#### CORS
```bash
# Origine autoris√©e pour les requ√™tes cross-origin
CORS_ORIGIN=http://localhost:3001

# Plusieurs origines (s√©par√©es par des virgules)
# CORS_ORIGIN=http://localhost:3001,https://myapp.com
```

#### llama.cpp et mod√®les
```bash
# Chemin vers l'ex√©cutable llama.cpp compil√©
LLAMA_CPP_PATH=/path/to/llama.cpp/main

# Param√®tres par d√©faut des mod√®les
PHI3_MAX_TOKENS=2048
PHI3_TEMPERATURE=0.7
```

#### Rate limiting
```bash
# Fen√™tre de temps en millisecondes (15 min = 900000)
RATE_LIMIT_WINDOW_MS=900000

# Nombre max de requ√™tes par fen√™tre
RATE_LIMIT_MAX_REQUESTS=100

# Pour production, r√©duire √† 50-100
# Pour d√©veloppement, augmenter √† 1000
```

### Exemple complet (.env)
```bash
# Serveur
PORT=3000
NODE_ENV=development

# JWT
JWT_SECRET=a1b2c3d4e5f6789abcdef0123456789abcdef0123456789
JWT_EXPIRES_IN=86400

# Base de donn√©es
DB_PATH=./database.sqlite

# CORS
CORS_ORIGIN=http://localhost:3001

# llama.cpp
LLAMA_CPP_PATH=/home/user/llama.cpp/main
PHI3_MAX_TOKENS=2048
PHI3_TEMPERATURE=0.7

# Rate limiting
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100
```

## llama.cpp

### Installation de llama.cpp

#### Linux/macOS
```bash
# Cloner le repository
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# Compiler avec toutes les optimisations
make clean
make -j$(nproc)

# Pour GPU NVIDIA (optionnel)
make LLAMA_CUBLAS=1

# Pour GPU AMD (optionnel)
make LLAMA_HIPBLAS=1

# Pour Apple Silicon (M1/M2)
make LLAMA_METAL=1

# V√©rifier l'installation
./main --help
```

#### Windows (avec MSYS2/MinGW)
```bash
# Installer MSYS2 depuis https://www.msys2.org/
# Dans MSYS2 terminal:
pacman -S mingw-w64-x86_64-gcc mingw-w64-x86_64-make git

# Cloner et compiler
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make
```

### Configuration du chemin
```bash
# Trouver le chemin complet de l'ex√©cutable
which llama.cpp  # ou whereis llama.cpp

# Exemple de chemins typiques :
# Linux: /home/user/llama.cpp/main
# macOS: /Users/user/llama.cpp/main
# Windows: C:\msys64\home\user\llama.cpp\main.exe

# Mettre √† jour .env
LLAMA_CPP_PATH=/chemin/vers/llama.cpp/main
```

### Test de llama.cpp
```bash
# Tester que llama.cpp fonctionne
/path/to/llama.cpp/main --help

# Doit afficher l'aide de llama.cpp
# Si erreur "Permission denied" :
chmod +x /path/to/llama.cpp/main
```

## Premier d√©marrage

### 1. Compilation TypeScript
```bash
# V√©rifier que TypeScript compile sans erreur
npm run type-check

# Si erreurs, les corriger avant de continuer
```

### 2. D√©marrage en mode d√©veloppement
```bash
# D√©marrer avec rechargement automatique
npm run dev

# Vous devriez voir :
# [INFO] Created models directory
# [INFO] Created data directory  
# [INFO] Created default preprompts
# [INFO] Database initialized successfully
# [INFO] Server running on port 3000
# [INFO] üöÄ Phi-3 Backend Simple is ready!
```

### 3. Test de base
```bash
# Tester que le serveur r√©pond
curl http://localhost:3000/health

# R√©ponse attendue :
# {"status":"healthy","timestamp":"...","uptime":123}
```

### 4. Test de llama.cpp
```bash
# Tester la connexion llama.cpp
curl -X POST http://localhost:3000/api/chat/test

# Si llama.cpp est configur√© correctement :
# {"success":true,"data":{"connectionStatus":"OK"},...}

# Si erreur :
# {"success":true,"data":{"connectionStatus":"FAILED"},...}
```

## D√©pannage

### Probl√®mes fr√©quents

#### 1. "Failed to start server"
```bash
# V√©rifier les logs complets
npm run dev 2>&1 | tee server.log

# Causes possibles :
# - Port d√©j√† utilis√©
# - Permissions de fichier
# - Variables d'environnement manquantes
```

**Solutions :**
```bash
# Port occup√©
lsof -i :3000  # Voir quel processus utilise le port
kill -9 PID   # Tuer le processus

# Ou changer le port
echo "PORT=3001" >> .env

# Permissions
chmod 755 .
chmod 644 .env
```

#### 2. "Database error"
```bash
# V√©rifier les permissions
ls -la database.sqlite

# Si probl√®me de permissions
sudo chown $USER:$USER database.sqlite
chmod 644 database.sqlite

# R√©initialiser la base (‚ö†Ô∏è perd les donn√©es)
rm database.sqlite
npm run dev  # Recr√©e automatiquement
```

#### 3. "llama.cpp not found"
```bash
# V√©rifier le chemin
ls -la $LLAMA_CPP_PATH

# Tester manuellement
$LLAMA_CPP_PATH --help

# Si erreur "command not found"
which llama.cpp
whereis llama
find /usr -name "*llama*" 2>/dev/null
```

**Solutions :**
```bash
# Recompiler llama.cpp
cd /path/to/llama.cpp
git pull
make clean
make -j$(nproc)

# Mettre √† jour le chemin
echo "LLAMA_CPP_PATH=$(pwd)/main" >> .env
```

#### 4. "Module not found"
```bash
# R√©installer les d√©pendances
rm -rf node_modules package-lock.json
npm install

# V√©rifier la version Node.js
node --version  # Doit √™tre >= 18

# Nettoyer le cache npm
npm cache clean --force
```

#### 5. "Permission denied"
```bash
# Probl√®mes de permissions Unix
sudo chown -R $USER:$USER .
chmod -R 755 .
chmod 644 .env package*.json

# Pour SQLite
mkdir -p data models
chmod 755 data models
```

### Logs de d√©bogage

#### Activer les logs d√©taill√©s
```bash
# Mode debug complet
DEBUG=* npm run dev

# Logs sp√©cifiques
DEBUG=express:* npm run dev      # Express seulement
DEBUG=sqlite:* npm run dev       # SQLite seulement
```

#### Analyser les logs
```bash
# Sauvegarder les logs
npm run dev 2>&1 | tee debug.log

# Filtrer les erreurs
grep -i error debug.log
grep -i warning debug.log
```

### Tests de connectivit√©

#### Test r√©seau
```bash
# V√©rifier que le port est ouvert
netstat -tlnp | grep :3000
ss -tlnp | grep :3000

# Test depuis une autre machine
curl -i http://IP_SERVER:3000/health
```

#### Test llama.cpp
```bash
# Test minimal avec un prompt court
$LLAMA_CPP_PATH -m /path/to/model.gguf -p "Hello" -n 10

# Si erreur "model not found", t√©l√©charger d'abord un mod√®le
```

### Performance et m√©moire

#### Monitoring des ressources
```bash
# Utilisation CPU/RAM
htop
top -p $(pgrep -f "node.*server.ts")

# Espace disque
df -h .
du -sh models/

# M√©moire du processus Node.js
ps aux | grep node
```

#### Optimisation
```bash
# Augmenter la limite de m√©moire Node.js
export NODE_OPTIONS="--max-old-space-size=4096"
npm run dev

# Ou dans package.json
"scripts": {
  "dev": "NODE_OPTIONS='--max-old-space-size=4096' ts-node-dev ..."
}
```

### Configuration avanc√©e

#### Variables d'environnement syst√®me
```bash
# Ajouter au .bashrc/.zshrc
export LLAMA_CPP_PATH="/usr/local/bin/llama-cpp"
export PHI3_MODELS_PATH="/var/lib/phi3-models"

# Recharger
source ~/.bashrc
```

#### Systemd (Linux production)
```bash
# Cr√©er /etc/systemd/system/phi3-backend.service
[Unit]
Description=Phi-3 Backend Simple
After=network.target

[Service]
Type=simple
User=phi3
WorkingDirectory=/opt/phi3-backend-simple
Environment=NODE_ENV=production
ExecStart=/usr/bin/npm start
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target

# Activer
sudo systemctl enable phi3-backend
sudo systemctl start phi3-backend
```

#### Docker (optionnel)
```dockerfile
FROM node:18-alpine

# Installer les d√©pendances syst√®me
RUN apk add --no-cache git build-base curl

# R√©pertoire de travail
WORKDIR /app

# Copier et installer les d√©pendances
COPY package*.json ./
RUN npm ci --only=production

# Copier le code source
COPY . .

# Compiler TypeScript
RUN npm run build

# Cr√©er un utilisateur non-root
RUN addgroup -g 1001 -S nodejs
RUN adduser -S phi3 -u 1001
USER phi3

# Exposer le port
EXPOSE 3000

# D√©marrer l'application
CMD ["npm", "start"]
```

### Support et aide

#### Logs utiles pour le support
```bash
# Informations syst√®me
uname -a
node --version
npm --version

# Configuration actuelle
cat .env
ls -la

# Logs d'erreur
tail -50 debug.log
```

#### O√π demander de l'aide
1. **Issues GitHub** : https://github.com/creach-t/phi3-backend-simple/issues
2. **Documentation llama.cpp** : https://github.com/ggerganov/llama.cpp
3. **Discord Node.js** : https://discord.gg/nodejs

#### Informations √† inclure dans un rapport de bug
- OS et version
- Version Node.js
- Logs d'erreur complets
- Configuration .env (sans les secrets)
- √âtapes pour reproduire

---

## ‚úÖ Check-list finale

Avant de passer en production, v√©rifiez :

- [ ] JWT_SECRET chang√© et s√©curis√©
- [ ] Base de donn√©es accessible et sauvegard√©e
- [ ] llama.cpp compil√© et test√©
- [ ] Mod√®les t√©l√©charg√©s et activ√©s
- [ ] Ports ouverts dans le firewall
- [ ] Monitoring et logs configur√©s
- [ ] Sauvegardes automatiques configur√©es
- [ ] Tests passent avec succ√®s
- [ ] Documentation √† jour

üéâ **Votre backend Phi-3 est maintenant pr√™t !**